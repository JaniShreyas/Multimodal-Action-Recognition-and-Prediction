{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.abspath(''), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: NVIDIA GeForce GTX 1660\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.io as io\n",
    "\n",
    "\n",
    "# Currently only for the rgb_frames along with narration\n",
    "class UCF50Dataset(Dataset):\n",
    "    def __init__(self, root_dir, annotations_csv, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations_df = pd.read_csv(annotations_csv)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        row = self.annotations_df.iloc[idx]\n",
    "\n",
    "        video_name, action, action_label = (\n",
    "            row[\"video_name\"],\n",
    "            row[\"action\"],\n",
    "            row[\"action_label\"],\n",
    "        )\n",
    "\n",
    "        video_frames = io.read_video(\n",
    "            os.path.join(self.root_dir, action, video_name),\n",
    "            pts_unit=\"sec\",\n",
    "            output_format=\"TCHW\",\n",
    "        )[0]\n",
    "\n",
    "        sample = {\"frames\": video_frames, \"action_label\": action_label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevConfig:\n",
    "    ROOT_DIR = r\"C:\\Users\\Jani\\.cache\\kagglehub\\datasets\\vineethakkinapalli\\ucf50-action-recognition-dataset\\versions\\1\\UCF50\"\n",
    "    ANNOTATIONS_DIR_LOCAL = \"annotations\"\n",
    "    FULL_ANNOTATIONS_FILE_LOCAL = \"annotations/annotations.csv\" \n",
    "    TRAIN_ANNOTATIONS_FILE_LOCAL = \"annotations/train_annotations.csv\"\n",
    "    VAL_ANNOTATIONS_FILE_LOCAL = \"annotations/val_annotations.csv\"\n",
    "    TEST_ANNOTATIONS_FILE_LOCAL = \"annotations/test_annotations.csv\"\n",
    "    MODELS_DIR_LOCAL = \"outputs/models\"\n",
    "    LOGS_DIR_LOCAL = \"outputs/logs\"\n",
    "    RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FixedSizeClipSampler(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom temporal sampler that:\n",
    "      1) Pads short clips (N < 32) by repeating the last frame until 32.\n",
    "      2) Uniformly subsamples long clips (N > 32) down to 32 frames.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_frames=32):\n",
    "        super().__init__()\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def forward(self, frames: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frames (torch.Tensor): A 4D tensor of shape (T, C, H, W)\n",
    "              - T: temporal dimension (# of frames)\n",
    "              - C: # of channels (3 for RGB)\n",
    "              - H, W: spatial dimensions\n",
    "\n",
    "        Returns:\n",
    "            A 4D tensor of shape (32, C, H, W) with fixed temporal length.\n",
    "        \"\"\"\n",
    "        t = frames.shape[0]\n",
    "\n",
    "        if t < self.num_frames:\n",
    "            # --- Pad short clips ---\n",
    "            pad_needed = self.num_frames - t\n",
    "            last_frame = frames[-1:].clone()  # shape (1, C, H, W)\n",
    "            # Repeat the last frame 'pad_needed' times and concatenate\n",
    "            pad_frames = last_frame.repeat(pad_needed, 1, 1, 1)\n",
    "            frames = torch.cat([frames, pad_frames], dim=0)\n",
    "\n",
    "        elif t > self.num_frames:\n",
    "            # --- Uniform subsampling for long clips ---\n",
    "            # Create 32 indices evenly spaced from [0..t-1]\n",
    "            indices = torch.linspace(0, t - 1, self.num_frames).long()\n",
    "            frames = frames[indices]\n",
    "\n",
    "        # If t == self.num_frames, we do nothing\n",
    "        return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformKey:\n",
    "    def __init__(self, key, transform):\n",
    "        self.key = key\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Apply the transform only on the designated key.\n",
    "        sample[self.key] = self.transform(sample[self.key])\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Lambda, CenterCrop, Normalize\n",
    "import os\n",
    "\n",
    "def get_x3d_model():\n",
    "    model_name = \"x3d_s\"\n",
    "    model = torch.hub.load(\"facebookresearch/pytorchvideo\", model_name, pretrained=True)\n",
    "\n",
    "    # Freeze parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.blocks[-1].proj.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    num_classes = len(\n",
    "        pd.read_csv(os.path.join(DevConfig.ANNOTATIONS_DIR_LOCAL, \"actions_label.csv\"))\n",
    "    )\n",
    "    model.blocks[-1].proj = nn.Linear(\n",
    "        in_features=model.blocks[-1].proj.in_features, out_features=num_classes\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def scale_pixels(x):\n",
    "    return x / 255.0\n",
    "\n",
    "def permute_tensor(x):\n",
    "    return x.permute(1, 0, 2, 3)\n",
    "\n",
    "def get_x3d_transform_compose():\n",
    "    mean = [0.45, 0.45, 0.45]\n",
    "    std = [0.225, 0.225, 0.225]\n",
    "    crop_size = 256\n",
    "    data_transform = Compose(\n",
    "        [\n",
    "            FixedSizeClipSampler(num_frames=32),\n",
    "            Lambda(scale_pixels),\n",
    "            Normalize(mean, std),\n",
    "            CenterCrop(crop_size),\n",
    "            Lambda(permute_tensor),\n",
    "        ]\n",
    "    )\n",
    "    data_transform = TransformKey(\"frames\", data_transform)\n",
    "\n",
    "    return data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def create_model_and_log_dir(model_dir, model_log_dir, experiment_name):\n",
    "    # Create the experiment directory if it doesn't already exist.\n",
    "    experiment_log_dir = os.path.join(model_log_dir, experiment_name)\n",
    "    os.makedirs(experiment_log_dir, exist_ok=True)\n",
    "\n",
    "    experiment_dir = os.path.join(model_dir, experiment_name)\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "    run_name = f\"run_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    # Generate a unique subdirectory name using the current timestamp.\n",
    "    run_log_dir = os.path.join(\n",
    "        experiment_log_dir, run_name\n",
    "    )\n",
    "    os.makedirs(run_log_dir, exist_ok=True)\n",
    "\n",
    "    run_model_name = os.path.join(experiment_name, run_name + \".pth\")\n",
    "\n",
    "    return run_model_name, run_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Jani/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n",
      "100%|██████████| 2338/2338 [15:46<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Accuracy: 77.07442258340463, Training Loss: 1.3141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [03:31<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 96.4072\n",
      "Validation Loss: 0.2146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2338/2338 [15:44<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Training Accuracy: 92.42942686056459, Training Loss: 0.4806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [03:21<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 97.3054\n",
      "Validation Loss: 0.1359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2338/2338 [14:28<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Training Accuracy: 94.05474764756202, Training Loss: 0.3458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [03:14<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 96.6068\n",
      "Validation Loss: 0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2338/2338 [16:07<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Training Accuracy: 94.3327630453379, Training Loss: 0.2951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [03:12<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 97.6048\n",
      "Validation Loss: 0.1012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2338/2338 [15:54<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Training Accuracy: 95.48759623609922, Training Loss: 0.2468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [03:15<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 98.3034\n",
      "Validation Loss: 0.0881\n",
      "Model saved to outputs/models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main():\n",
    "    # Load the official SlowFast model from PyTorch Hub.\n",
    "    model = get_x3d_model()\n",
    "\n",
    "    model_name, log_dir = create_model_and_log_dir(DevConfig.MODELS_DIR_LOCAL, DevConfig.LOGS_DIR_LOCAL, \"X3D_only_freeze\")\n",
    "\n",
    "    # Create Tensorboard summary writer for logging\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    data_transform = get_x3d_transform_compose()\n",
    "    train_dataset = UCF50Dataset(\n",
    "        DevConfig.ROOT_DIR,\n",
    "        DevConfig.TRAIN_ANNOTATIONS_FILE_LOCAL,\n",
    "        transform=data_transform,\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=2, shuffle=True\n",
    "    )\n",
    "\n",
    "    val_dataset = UCF50Dataset(\n",
    "        DevConfig.ROOT_DIR,\n",
    "        DevConfig.VAL_ANNOTATIONS_FILE_LOCAL,\n",
    "        transform=data_transform,\n",
    "    )\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    num_epochs = 5\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        batch_count = 0\n",
    "        model.train()\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            input = batch[\"frames\"].to(device)\n",
    "            batch_count += 1\n",
    "\n",
    "            labels = batch[\"action_label\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input)\n",
    "\n",
    "            predictions = outputs.argmax(dim = 1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            global_step = epoch * len(train_dataloader) + batch_count\n",
    "            \n",
    "            writer.add_scalar(\"Loss/Train_Batch\", loss.item(), global_step)\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_dataloader)\n",
    "        accuracy = (correct_predictions/len(train_dataset)) * 100\n",
    "        writer.add_scalar(\"Loss/Train_Epoch\", avg_train_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Accuracy: {accuracy}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader):\n",
    "                input = batch[\"frames\"].to(device)\n",
    "                labels = batch[\"action_label\"].to(device)\n",
    "                outputs = model(input)\n",
    "\n",
    "                predictions = outputs.argmax(dim = 1)\n",
    "                correct_predictions += (predictions == labels).sum().item()\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = valid_loss / len(val_dataloader)\n",
    "        accuracy = (correct_predictions/len(val_dataset)) * 100\n",
    "        writer.add_scalar(\"Loss/Valid_Epoch\", avg_val_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Valid_Epoch\", accuracy, epoch)\n",
    "\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # After finishing training\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        f\"{DevConfig.MODELS_DIR_LOCAL}/{model_name}\",\n",
    "    )\n",
    "    print(f\"Model saved to {DevConfig.MODELS_DIR_LOCAL}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Jani/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Load test dataset\n",
    "    test_dataset = UCF50Dataset(\n",
    "        DevConfig.ROOT_DIR,\n",
    "        DevConfig.TEST_ANNOTATIONS_FILE_LOCAL,\n",
    "        transform = get_x3d_transform_compose()\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=2, shuffle=True\n",
    "    )\n",
    "\n",
    "    model = get_x3d_model()\n",
    "    model.load_state_dict(torch.load(os.path.join(DevConfig.MODELS_DIR_LOCAL, \"X3D_only_freeze/run_20250422-215343.pth\"), map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate on the test dataset\n",
    "    total_samples = len(test_dataset)\n",
    "    correct_predictions = 0\n",
    "    inference_times = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs = batch[\"frames\"].to(device)\n",
    "            labels = batch[\"action_label\"].to(device)\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            predictions = outputs.argmax(dim = 1)\n",
    "\n",
    "            end_time = time.perf_counter()\n",
    "            inference_times.append(end_time - start_time)\n",
    "\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "    \n",
    "    accuracy = (correct_predictions/total_samples) * 100\n",
    "    avg_inference_time = sum(inference_times) / len(test_dataset)\n",
    "\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(accuracy))\n",
    "    print(f\"Average Inference Time per sample: {avg_inference_time:.4f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
